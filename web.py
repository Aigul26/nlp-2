import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from gensim.models import Word2Vec, FastText, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap.umap_ as umap
import random
from typing import List, Dict, Any

class VectorSpaceExplorer:
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤"""
    
    def __init__(self):
        self.models = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        self.current_model = None
        self.current_model_name = None
        self.vocab = []
        
        # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –¥–µ–º–æ-–º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞
        self._create_all_demo_models()
    
    def _create_all_demo_models(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            sentences = self._generate_training_sentences()
            
            st.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            progress_bar = st.progress(0)
            status_text = st.empty()
            # 1. Word2Vec Skip-gram
            status_text.text("–°–æ–∑–¥–∞–Ω–∏–µ Word2Vec Skip-gram...")
            self.models['word2vec_sg'] = Word2Vec(
                sentences=sentences,
                vector_size=150,
                window=8,
                min_count=1,
                workers=4,
                epochs=50,
                sg=1  # skip-gram
            )
            progress_bar.progress(20)
            
            # 2. Word2Vec CBOW
            status_text.text("–°–æ–∑–¥–∞–Ω–∏–µ Word2Vec CBOW...")
            self.models['word2vec_cbow'] = Word2Vec(
                sentences=sentences,
                vector_size=150,
                window=8,
                min_count=1,
                workers=4,
                epochs=50,
                sg=0  # CBOW
            )
            progress_bar.progress(40)
            
            # 3. FastText Skip-gram
            status_text.text("–°–æ–∑–¥–∞–Ω–∏–µ FastText Skip-gram...")
            self.models['fasttext_sg'] = FastText(
                sentences=sentences,
                vector_size=150,
                window=8,
                min_count=1,
                workers=4,
                epochs=50,
                sg=1  # skip-gram
            )
            progress_bar.progress(60)
            
            # 4. Doc2Vec PV-DM (Distributed Memory)
            status_text.text("–°–æ–∑–¥–∞–Ω–∏–µ Doc2Vec PV-DM...")
            tagged_documents = [TaggedDocument(words=doc, tags=[f'doc_{i}']) 
                              for i, doc in enumerate(sentences)]
            
            self.models['doc2vec_dm'] = Doc2Vec(
                documents=tagged_documents,
                vector_size=150,
                window=8,
                min_count=1,
                workers=4,
                epochs=50,
                dm=1  # PV-DM
            )
            progress_bar.progress(80)
            
            # 5. Doc2Vec PV-DBOW (Distributed Bag of Words)
            status_text.text("–°–æ–∑–¥–∞–Ω–∏–µ Doc2Vec PV-DBOW...")
            self.models['doc2vec_dbow'] = Doc2Vec(
                documents=tagged_documents,
                vector_size=150,
                window=8,
                min_count=1,
                workers=4,
                epochs=50,
                dm=0  # PV-DBOW
            )
            progress_bar.progress(100)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.current_model = self.models['word2vec_sg']
            self.current_model_name = "Word2Vec Skip-gram"
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å
            self._create_combined_vocabulary()
            
            status_text.text("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã!")
            st.success(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.models)} –º–æ–¥–µ–ª–µ–π! –°–ª–æ–≤–∞—Ä—å: {len(self.vocab)} —Å–ª–æ–≤")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π —Å—Ç–∞—Ç—É—Å–∞
            import time
            time.sleep(2)
            status_text.empty()
            progress_bar.empty()
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
            self._create_fallback_vocabulary()
    
    def _generate_training_sentences(self) -> List[List[str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        sentences = []
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –≥–æ—Ä–æ–¥–∞
        geo_sentences = [
            ["—Ä–æ—Å—Å–∏—è", "–º–æ—Å–∫–≤–∞", "—Å—Ç–æ–ª–∏—Ü–∞", "–≥–æ—Ä–æ–¥", "–∫—Ä–µ–º–ª—å", "—Ä–µ–∫–∞", "–≤–æ–ª–≥–∞"],
            ["—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–∏—Ç–µ—Ä", "–≥–æ—Ä–æ–¥", "–Ω–µ–≤–∞", "—ç—Ä–º–∏—Ç–∞–∂", "–∫—É–ª—å—Ç—É—Ä–∞"],
            ["–ø–∞—Ä–∏–∂", "—Ñ—Ä–∞–Ω—Ü–∏—è", "–µ–≤—Ä–æ–ø–∞", "–≥–æ—Ä–æ–¥", "–ª—É–≤—Ä", "—ç–π—Ñ–µ–ª–µ–≤–∞", "–±–∞—à–Ω—è"],
            ["–Ω—å—é-–π–æ—Ä–∫", "—Å—à–∞", "–∞–º–µ—Ä–∏–∫–∞", "–≥–æ—Ä–æ–¥", "–Ω–µ–±–æ—Å–∫—Ä–µ–±", "—Å—Ç–∞—Ç—É—è", "—Å–≤–æ–±–æ–¥—ã"],
            ["–∫–∏—Ç–∞–π", "–ø–µ–∫–∏–Ω", "–∞–∑–∏—è", "—Å—Ç—Ä–∞–Ω–∞", "–≤–µ–ª–∏–∫–∞—è", "—Å—Ç–µ–Ω–∞", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
            ["—è–ø–æ–Ω–∏—è", "—Ç–æ–∫–∏–æ", "–∞–∑–∏—è", "—Å—Ç—Ä–∞–Ω–∞", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Å–∞–∫—É—Ä–∞", "–∫—É–ª—å—Ç—É—Ä–∞"],
            ["–≥–µ—Ä–º–∞–Ω–∏—è", "–±–µ—Ä–ª–∏–Ω", "–µ–≤—Ä–æ–ø–∞", "—Å—Ç—Ä–∞–Ω–∞", "–∞–≤—Ç–æ–º–æ–±–∏–ª–∏", "–±–º–≤", "–º–µ—Ä—Å–µ–¥–µ—Å"],
            ["–∏—Ç–∞–ª–∏—è", "—Ä–∏–º", "–µ–≤—Ä–æ–ø–∞", "—Å—Ç—Ä–∞–Ω–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–ø–∏—Ü—Ü–∞", "–ø–∞—Å—Ç–∞"],
            ["–∞–Ω–≥–ª–∏—è", "–ª–æ–Ω–¥–æ–Ω", "–µ–≤—Ä–æ–ø–∞", "—Å—Ç—Ä–∞–Ω–∞", "–∫–æ—Ä–æ–ª–µ–≤–∞", "–±–∏–≥-–±–µ–Ω", "—Ç—Ä–∞–¥–∏—Ü–∏–∏"],
            ["–∏—Å–ø–∞–Ω–∏—è", "–º–∞–¥—Ä–∏–¥", "–µ–≤—Ä–æ–ø–∞", "—Å—Ç—Ä–∞–Ω–∞", "—Ñ—É—Ç–±–æ–ª", "–∫–æ—Ä—Ä–∏–¥–∞", "—Ç–∞–Ω—Ü—ã"]
        ]
        
        # –ü–æ–ª–∏—Ç–∏–∫–∞ –∏ –≤–ª–∞—Å—Ç—å
        politics_sentences = [
            ["–ø—É—Ç–∏–Ω", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–≤–ª–∞—Å—Ç—å", "–∫—Ä–µ–º–ª—å", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–ø–æ–ª–∏—Ç–∏–∫–∞"],
            ["–±–∞–π–¥–µ–Ω", "—Å—à–∞", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–∞–º–µ—Ä–∏–∫–∞", "–±–µ–ª—ã–π", "–¥–æ–º", "–¥–µ–º–æ–∫—Ä–∞—Ç–∏—è"],
            ["–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ", "–±—é–¥–∂–µ—Ç", "–Ω–∞–ª–æ–≥–∏", "–∑–∞–∫–æ–Ω—ã", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ"],
            ["–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "–¥—É–º–∞", "–¥–µ–ø—É—Ç–∞—Ç—ã", "–∑–∞–∫–æ–Ω—ã", "–≤—ã–±–æ—Ä—ã", "–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ"],
            ["–æ–ø–ø–æ–∑–∏—Ü–∏—è", "–ø—Ä–æ—Ç–µ—Å—Ç", "–º–∏—Ç–∏–Ω–≥", "–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–≤–ª–∞—Å—Ç—å"],
            ["–¥–∏–ø–ª–æ–º–∞—Ç–∏—è", "–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã", "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ", "–æ—Ç–Ω–æ—à–µ–Ω–∏—è", "–ø–æ—Å–æ–ª—å—Å—Ç–≤–æ"],
            ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è", "—Ç–æ—Ä–≥–æ–≤–ª—è"],
            ["–≤–æ–π–Ω–∞", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç", "–∞—Ä–º–∏—è", "—Å–æ–ª–¥–∞—Ç—ã", "–æ—Ä—É–∂–∏–µ", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"]
        ]
        
        # –ö—É–ª—å—Ç—É—Ä–∞ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ
        culture_sentences = [
            ["–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "–∞–∫—Ç–µ—Ä", "—Ä–µ–∂–∏—Å—Å–µ—Ä", "—Ñ–µ—Å—Ç–∏–≤–∞–ª—å", "–ø—Ä–µ–º–∏—è", "–æ—Å–∫–∞—Ä"],
            ["–∫—É–ª—å—Ç—É—Ä–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–º—É–∑–µ–π", "—Ç–µ–∞—Ç—Ä", "–≤—ã—Å—Ç–∞–≤–∫–∞", "–∫–∞—Ä—Ç–∏–Ω–∞", "—Å–∫—É–ª—å–ø—Ç—É—Ä–∞"],
            ["–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞", "–∫–Ω–∏–≥–∞", "–ø–∏—Å–∞—Ç–µ–ª—å", "–ø–æ—ç—Ç", "—Ä–æ–º–∞–Ω", "—Å—Ç–∏—Ö–∏", "–ø—Ä–æ–∑–∞"],
            ["–º—É–∑—ã–∫–∞", "–ø–µ—Å–Ω—è", "–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å", "–∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∞–ª—å–±–æ–º"],
            ["—Ç–∞–Ω–µ—Ü", "–±–∞–ª–µ—Ç", "—Ö–æ—Ä–µ–æ–≥—Ä–∞—Ñ–∏—è", "–¥–≤–∏–∂–µ–Ω–∏–µ", "—Ä–∏—Ç–º", "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ"],
            ["–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–∑–¥–∞–Ω–∏–µ", "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–¥–∏–∑–∞–π–Ω", "–ø—Ä–æ–µ–∫—Ç", "—á–µ—Ä—Ç–µ–∂"],
            ["—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è", "–∫–∞–º–µ—Ä–∞", "—Å–Ω–∏–º–æ–∫", "–æ–±—ä–µ–∫—Ç–∏–≤", "–∫–æ–º–ø–æ–∑–∏—Ü–∏—è", "—Å–≤–µ—Ç"],
            ["–∂–∏–≤–æ–ø–∏—Å—å", "—Ö—É–¥–æ–∂–Ω–∏–∫", "–∫—Ä–∞—Å–∫–∏", "–ø–æ–ª–æ—Ç–Ω–æ", "–ø–µ–π–∑–∞–∂", "–ø–æ—Ä—Ç—Ä–µ—Ç"]
        ]
        
        # –≠–∫–æ–Ω–æ–º–∏–∫–∞ –∏ –±–∏–∑–Ω–µ—Å
        economy_sentences = [
            ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–¥–µ–Ω—å–≥–∏", "—Ä—É–±–ª—å", "–¥–æ–ª–ª–∞—Ä", "–±–∏–∑–Ω–µ—Å", "–∫–æ–º–ø–∞–Ω–∏—è", "—Ä—ã–Ω–æ–∫"],
            ["–±–∞–Ω–∫", "–∫—Ä–µ–¥–∏—Ç", "–≤–∫–ª–∞–¥", "–ø—Ä–æ—Ü–µ–Ω—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Å—á–µ—Ç"],
            ["–∏–Ω—Ñ–ª—è—Ü–∏—è", "—Ü–µ–Ω—ã", "—Ä–æ—Å—Ç", "–ø–∞–¥–µ–Ω–∏–µ", "–∫—É—Ä—Å", "–≤–∞–ª—é—Ç—ã", "–æ–±–º–µ–Ω"],
            ["–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–∫–∞–ø–∏—Ç–∞–ª", "–ø—Ä–∏–±—ã–ª—å", "—É–±—ã—Ç–æ–∫", "–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥"],
            ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—Ä–µ—Å—É—Ä—Å—ã", "–¥–æ–±—ã—á–∞", "—ç–Ω–µ—Ä–≥–∏—è", "—Ç–æ–ø–ª–∏–≤–æ", "—ç–∫—Å–ø–æ—Ä—Ç"],
            ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏", "—Å—Ç–∞—Ä—Ç–∞–ø", "–≤–µ–Ω—á—É—Ä–Ω—ã–π", "–∫–∞–ø–∏—Ç–∞–ª", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"],
            ["—Ç–æ—Ä–≥–æ–≤–ª—è", "–º–∞–≥–∞–∑–∏–Ω", "–ø–æ–∫—É–ø–∫–∞", "–ø—Ä–æ–¥–∞–∂–∞", "–∫–ª–∏–µ–Ω—Ç", "—É—Å–ª—É–≥–∞", "—Ç–æ–≤–∞—Ä"],
            ["—Ä–∞–±–æ—Ç–∞", "–∑–∞—Ä–ø–ª–∞—Ç–∞", "–∫–∞—Ä—å–µ—Ä–∞", "–ø—Ä–æ—Ñ–µ—Å—Å–∏—è", "–Ω–∞–≤—ã–∫–∏", "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"]
        ]
        
        # –ù–∞—É–∫–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        science_sentences = [
            ["–Ω–∞—É–∫–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—É—á–µ–Ω—ã–π", "–æ—Ç–∫—Ä—ã—Ç–∏–µ", "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç"],
            ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "—Å—Ç—É–¥–µ–Ω—Ç", "–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å", "–ª–µ–∫—Ü–∏—è", "—ç–∫–∑–∞–º–µ–Ω"],
            ["—à–∫–æ–ª–∞", "—É—á–∏—Ç–µ–ª—å", "—É—á–µ–Ω–∏–∫", "—É—Ä–æ–∫", "–¥–æ–º–∞—à–Ω–µ–µ", "–∑–∞–¥–∞–Ω–∏–µ", "–æ—Ü–µ–Ω–∫–∞"],
            ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∞–ª–≥–æ—Ä–∏—Ç–º", "–¥–∞–Ω–Ω—ã–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"],
            ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–≤—Ä–∞—á", "–±–æ–ª—å–Ω–∏—Ü–∞", "–ª–µ—á–µ–Ω–∏–µ", "–¥–∏–∞–≥–Ω–æ–∑", "–∑–¥–æ—Ä–æ–≤—å–µ", "–ø–∞—Ü–∏–µ–Ω—Ç"],
            ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "—á–∏—Å–ª–∞", "—Ñ–æ—Ä–º—É–ª—ã", "—É—Ä–∞–≤–Ω–µ–Ω–∏—è", "—Ç–µ–æ—Ä–µ–º–∞", "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ"],
            ["—Ñ–∏–∑–∏–∫–∞", "–∞—Ç–æ–º", "—ç–Ω–µ—Ä–≥–∏—è", "–∑–∞–∫–æ–Ω—ã", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"],
            ["—Ö–∏–º–∏—è", "—ç–ª–µ–º–µ–Ω—Ç—ã", "—Ä–µ–∞–∫—Ü–∏–∏", "–º–æ–ª–µ–∫—É–ª—ã", "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è", "–æ–ø—ã—Ç—ã"]
        ]
        
        # –°–ø–æ—Ä—Ç
        sport_sentences = [
            ["—Å–ø–æ—Ä—Ç", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "–∏–≥—Ä–∞", "–∫–æ–º–∞–Ω–¥–∞", "—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ", "–ø–æ–±–µ–¥–∞"],
            ["–æ–ª–∏–º–ø–∏–∞–¥–∞", "–º–µ–¥–∞–ª—å", "—á–µ–º–ø–∏–æ–Ω–∞—Ç", "—Ä–µ–∫–æ—Ä–¥", "–∞—Ç–ª–µ—Ç", "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞"],
            ["–±–∞—Å–∫–µ—Ç–±–æ–ª", "–º—è—á", "–∫–æ—Ä–∑–∏–Ω–∞", "–ø–ª–æ—â–∞–¥–∫–∞", "–∫–æ–º–∞–Ω–¥–∞", "–æ—á–∫–∏"],
            ["—Ç–µ–Ω–Ω–∏—Å", "—Ä–∞–∫–µ—Ç–∫–∞", "–º—è—á", "—Å–µ—Ç", "–ø–æ–¥–∞—á–∞", "—Ç—É—Ä–Ω–∏—Ä"],
            ["–ø–ª–∞–≤–∞–Ω–∏–µ", "–±–∞—Å—Å–µ–π–Ω", "–≤–æ–¥–∞", "—Å—Ç–∏–ª—å", "–¥–∏—Å—Ç–∞–Ω—Ü–∏—è", "—Ä–µ–∫–æ—Ä–¥"],
            ["–±–æ–∫—Å", "—Ä–∏–Ω–≥", "–ø–µ—Ä—á–∞—Ç–∫–∏", "–ø–æ–µ–¥–∏–Ω–æ–∫", "–Ω–æ–∫–∞—É—Ç", "—á–µ–º–ø–∏–æ–Ω"],
            ["–∞–≤—Ç–æ—Å–ø–æ—Ä—Ç", "–≥–æ–Ω–∫–∏", "—Ç—Ä–∞—Å—Å–∞", "—Å–∫–æ—Ä–æ—Å—Ç—å", "–ø–∏–ª–æ—Ç", "–ø–æ–±–µ–¥–∞"],
            ["—à–∞—Ö–º–∞—Ç—ã", "–¥–æ—Å–∫–∞", "—Ñ–∏–≥—É—Ä—ã", "—Ö–æ–¥", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "—Ç—É—Ä–Ω–∏—Ä"]
        ]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        all_sentences = (geo_sentences + politics_sentences + culture_sentences + 
                       economy_sentences + science_sentences + sport_sentences)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è
        for sentence in all_sentences:
            sentences.append(sentence)
            # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
            if "—Ä–æ—Å—Å–∏—è" in sentence:
                sentences.append([w.replace("—Ä–æ—Å—Å–∏—è", "—Ä–æ–¥–∏–Ω–∞") if w == "—Ä–æ—Å—Å–∏—è" else w for w in sentence])
            if "–≥–æ—Ä–æ–¥" in sentence:
                sentences.append([w.replace("–≥–æ—Ä–æ–¥", "–º–µ–≥–∞–ø–æ–ª–∏—Å") if w == "–≥–æ—Ä–æ–¥" else w for w in sentence])
            if "—Å—Ç—Ä–∞–Ω–∞" in sentence:
                sentences.append([w.replace("—Å—Ç—Ä–∞–Ω–∞", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ") if w == "—Å—Ç—Ä–∞–Ω–∞" else w for w in sentence])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–π —Å–µ–º–∞–Ω—Ç–∏–∫–∏
        for _ in range(500):
            base1 = random.choice(all_sentences)
            base2 = random.choice(all_sentences)
            if random.random() > 0.7:  # 30% chance to combine
                new_sentence = list(set(base1[:3] + base2[:3]))
                if len(new_sentence) >= 3:
                    sentences.append(new_sentence)
        
        return sentences
    
    def _create_combined_vocabulary(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∏–∑ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        all_words = set()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Word2Vec –∏ FastText
        for model_name, model in self.models.items():
            if hasattr(model, 'wv') and hasattr(model.wv, 'key_to_index'):
                all_words.update(model.wv.key_to_index.keys())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞
        self.vocab = [word for word in all_words 
                     if len(word) > 2 and word.isalpha() and not word.isdigit()]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        self.vocab.sort()
    
    def _create_fallback_vocabulary(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è"""
        self.vocab = [
            "—Ä–æ—Å—Å–∏—è", "–º–æ—Å–∫–≤–∞", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–∫–∞–∑–∞–Ω—å",
            "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥", "—á–µ–ª—è–±–∏–Ω—Å–∫", "—Å–∞–º–∞—Ä–∞", "–æ–º—Å–∫", "—Ä–æ—Å—Ç–æ–≤", "—É—Ñ–∞", "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫",
            "–ø–µ—Ä–º—å", "–≤–æ—Ä–æ–Ω–µ–∂", "–≤–æ–ª–≥–æ–≥—Ä–∞–¥", "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä", "—Å–∞—Ä–∞—Ç–æ–≤", "—Ç—é–º–µ–Ω—å", "–∏–∂–µ–≤—Å–∫",
            "–±–∞—Ä–Ω–∞—É–ª", "—É–ª—å—è–Ω–æ–≤—Å–∫", "–≤–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫", "—è—Ä–æ—Å–ª–∞–≤–ª—å", "–∏—Ä–∫—É—Ç—Å–∫", "—Ç–æ–º—Å–∫", "–æ—Ä–µ–Ω–±—É—Ä–≥",
            "–∫–µ–º–µ—Ä–æ–≤–æ", "–Ω–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫", "—Ä—è–∑–∞–Ω—å", "–∞—Å—Ç—Ä–∞—Ö–∞–Ω—å", "–ø–µ–Ω–∑–∞", "–ª–∏–ø–µ—Ü–∫", "–∫–∏—Ä–æ–≤",
            "—á–µ–±–æ–∫—Å–∞—Ä—ã", "–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥", "–∫—É—Ä—Å–∫", "—Ç–≤–µ—Ä—å", "—Å—Ç–∞–≤—Ä–æ–ø–æ–ª—å", "–º–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫",
            "—Å–æ—á–∏", "—Ç—É–ª–∞", "–±—Ä—è–Ω—Å–∫", "–±–µ–ª–≥–æ—Ä–æ–¥", "–∫—É—Ä–≥–∞–Ω", "–∞—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫", "–≤–ª–∞–¥–∏–º–∏—Ä",
            "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "—Å—É—Ä–≥—É—Ç", "—á–µ–±–æ–∫—Å–∞—Ä—ã", "–≤–æ–ª–æ–≥–¥–∞", "—Å–∞—Ä–∞–Ω—Å–∫",
            "—á–µ–±–æ–∫—Å–∞—Ä—ã", "–º—É—Ä–º–∞–Ω—Å–∫", "–∫–∞–ª—É–≥–∞", "–æ—Ä—ë–ª", "—Å–º–æ–ª–µ–Ω—Å–∫", "—á–∏—Ç–∞", "–≤–ª–∞–¥–∏–∫–∞–≤–∫–∞–∑",
            "—è–∫—É—Ç—Å–∫", "—Ö–∞—Ä—å–∫–æ–≤", "–∫–∏–µ–≤", "–º–∏–Ω—Å–∫", "–∞—Å—Ç–∞–Ω–∞", "—Ç–∞—à–∫–µ–Ω—Ç", "–±–∞–∫—É", "–µ—Ä–µ–≤–∞–Ω",
            "—Ç–±–∏–ª–∏—Å–∏", "–≤–∏–ª—å–Ω—é—Å", "—Ä–∏–≥–∞", "—Ç–∞–ª–ª–∏–Ω", "–≤–∞—Ä—à–∞–≤–∞", "–ø—Ä–∞–≥–∞", "–±—É–¥–∞–ø–µ—à—Ç", "–±—É—Ö–∞—Ä–µ—Å—Ç",
            "—Å–æ—Ñ–∏—è", "–±–µ–ª–≥—Ä–∞–¥", "–∑–∞–≥—Ä–µ–±", "—Å–∞—Ä–∞–µ–≤–æ", "–ø–æ–¥–≥–æ—Ä–∏—Ü–∞", "–ø—Ä–∏—à—Ç–∏–Ω–∞", "–∫–∏—à–∏–Ω–µ–≤",
            "–∫–∏—à–∏–Ω–µ–≤", "—Ç–æ–∫–∏–æ", "–ø–µ–∫–∏–Ω", "—Å–µ—É–ª", "–±–∞–Ω–≥–∫–æ–∫", "–¥–∂–∞–∫–∞—Ä—Ç–∞", "–º–∞–Ω–∏–ª–∞", "—Ö–∞–Ω–æ–π",
            "–∫—É–∞–ª–∞-–ª—É–º–ø—É—Ä", "—Å–∏–Ω–≥–∞–ø—É—Ä", "—Ç–∞–π–±—ç–π", "–≥–æ–Ω–∫–æ–Ω–≥", "–º–∞–∫–∞–æ", "–¥–µ–±—Ä–µ–π—Ç", "–∫–∞—Ç–º–∞–Ω–¥—É",
            "–∫–æ–ª–æ–º–±–æ", "–¥–∞–∫–∫–∞", "–∏—Å–ª–∞–º–∞–±–∞–¥", "–∫–∞–±—É–ª", "—Ç–µ–≥–µ—Ä–∞–Ω", "–±–∞–≥–¥–∞–¥", "—ç—Ä—å-—Ä–∏—è–¥", "–¥—É–±–∞–π",
            "–∫–∞–∏—Ä", "—Ä–∞–±–∞—Ç", "–∞–ª–∂–∏—Ä", "—Ç—É–Ω–∏—Å", "—Ç—Ä–∏–ø–æ–ª–∏", "—Ö–∞—Ä–∞—Ä–µ", "–¥–∞–∫–∞—Ä", "–ª–∞–≥–æ—Å", "–∞–∫–∫—Ä–∞",
            "–Ω–∞–π—Ä–æ–±–∏", "–∞–¥–¥–∏—Å-–∞–±–µ–±–∞", "–∞–Ω—Ç–∞–Ω–∞–Ω–∞—Ä–∏–≤—É", "–∫–∞–º–ø–∞–ª–∞", "–¥–∞—Ä–µ—Å-—Å–∞–ª–∞–º", "–ª—É–∞–Ω–¥–∞",
            "–∫–∏–Ω—à–∞—Å–∞", "–∞–±—É–¥–∂–∞", "–±–∞–º–∞–∫–æ", "—É–∂–≥–æ—Ä–æ–¥", "—É–∂–≥–æ—Ä–æ–¥", "—É–∂–≥–æ—Ä–æ–¥", "—É–∂–≥–æ—Ä–æ–¥"
        ]
    
    def render_sidebar(self):
        """–ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        st.sidebar.title("üîç –ê–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        st.sidebar.markdown("### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
        
        model_options = {
            "Word2Vec Skip-gram": "word2vec_sg",
            "Word2Vec CBOW": "word2vec_cbow", 
            "FastText Skip-gram": "fasttext_sg",
            "Doc2Vec PV-DM": "doc2vec_dm",
            "Doc2Vec PV-DBOW": "doc2vec_dbow"
        }
        
        selected_model_name = st.sidebar.selectbox(
            "–ú–æ–¥–µ–ª—å:",
            list(model_options.keys()),
            index=0
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        model_key = model_options[selected_model_name]
        if model_key in self.models:
            self.current_model = self.models[model_key]
            self.current_model_name = selected_model_name
        
        if self.current_model:
            st.sidebar.success(f"‚úÖ {self.current_model_name}")
            st.sidebar.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self._get_model_vector_size()}D")
            st.sidebar.info(f"–°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(self.vocab)}")
        else:
            st.sidebar.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        st.sidebar.markdown("---")
        st.sidebar.markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        
        viz_method = st.sidebar.selectbox(
            "–ú–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:",
            ["t-SNE", "PCA", "UMAP"]
        )
        
        num_words = st.sidebar.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:",
            min_value=50,
            max_value=1000,
            value=300
        )
        
        return viz_method, num_words
    
    def _get_model_vector_size(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        if not self.current_model:
            return 0
        
        if hasattr(self.current_model, 'vector_size'):
            return self.current_model.vector_size
        elif hasattr(self.current_model, 'wv') and hasattr(self.current_model.wv, 'vector_size'):
            return self.current_model.wv.vector_size
        else:
            return 0
    
    def _get_word_vector(self, word: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        if not self.current_model:
            return None
        
        try:
            # –î–ª—è Word2Vec –∏ FastText
            if hasattr(self.current_model, 'wv'):
                return self.current_model.wv[word]
            # –î–ª—è Doc2Vec (—Ä–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏)
            elif hasattr(self.current_model, 'dv'):
                return self.current_model.dv[word]
            else:
                return self.current_model[word]
        except:
            return None
    
    def _word_in_vocabulary(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        if not self.current_model:
            return False
        
        try:
            # –î–ª—è Word2Vec –∏ FastText
            if hasattr(self.current_model, 'wv') and hasattr(self.current_model.wv, 'key_to_index'):
                return word in self.current_model.wv.key_to_index
            # –î–ª—è Doc2Vec
            elif hasattr(self.current_model, 'dv') and hasattr(self.current_model.dv, 'key_to_index'):
                return word in self.current_model.dv.key_to_index
            else:
                return word in self.current_model.key_to_index
        except:
            return False
    
    def _get_most_similar(self, word: str, topn: int = 10):
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤"""
        if not self.current_model:
            return None
        
        try:
            # –î–ª—è Word2Vec –∏ FastText
            if hasattr(self.current_model, 'wv'):
                return self.current_model.wv.most_similar(word, topn=topn)
            # –î–ª—è Doc2Vec
            elif hasattr(self.current_model, 'dv'):
                return self.current_model.dv.most_similar(word, topn=topn)
            else:
                return self.current_model.most_similar(word, topn=topn)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤: {e}")
            return None
    
    def _compute_similarity(self, word1: str, word2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not self.current_model:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        try:
            # –î–ª—è Word2Vec –∏ FastText
            if hasattr(self.current_model, 'wv'):
                return self.current_model.wv.similarity(word1, word2)
            # –î–ª—è Doc2Vec
            elif hasattr(self.current_model, 'dv'):
                return self.current_model.dv.similarity(word1, word2)
            else:
                return self.current_model.similarity(word1, word2)
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
    
    def render_vector_arithmetic(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏"""
        st.header("üßÆ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞")
        
        if not self.current_model:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.")
            return
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("### –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –∞–Ω–∞–ª–æ–≥–∏–π")
            expression = st.text_input(
                "–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ:",
                value="—Ä–æ—Å—Å–∏—è - –º–æ—Å–∫–≤–∞ + –ø–∞—Ä–∏–∂",
                help="–§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1 - —Å–ª–æ–≤–æ2 + —Å–ª–æ–≤–æ3"
            )
            
            topn = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:", 1, 20, 10, key="arithmetic_topn")
            
            if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å", type="primary", key="calc_btn"):
                if expression:
                    with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ..."):
                        try:
                            result = self._compute_vector_arithmetic(expression, topn)
                            if result:
                                self._display_arithmetic_results(expression, result)
                            else:
                                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –≤—ã—Ä–∞–∂–µ–Ω–∏–µ")
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞: {e}")
        
        with col2:
            st.markdown("### –ü—Ä–∏–º–µ—Ä—ã")
            examples = [
                "—Ä–æ—Å—Å–∏—è - –º–æ—Å–∫–≤–∞ + –ø–∞—Ä–∏–∂",
                "–ø—É—Ç–∏–Ω - —Ä–æ—Å—Å–∏—è + —Å—à–∞",
                "—Ä—É–±–ª—å - —Ä–æ—Å—Å–∏—è + –¥–æ–ª–ª–∞—Ä",
                "–∫–∏–Ω–æ - —Ä–æ—Å—Å–∏—è + —Ñ—Ä–∞–Ω—Ü–∏—è",
                "—Ñ—É—Ç–±–æ–ª - —Ä–æ—Å—Å–∏—è + –±—Ä–∞–∑–∏–ª–∏—è"
            ]
            
            for example in examples:
                if st.button(example, key=f"ex_{hash(example)}"):
                    st.session_state.expression = example
    
    def _compute_vector_arithmetic(self, expression, topn=10):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏"""
        if not self.current_model:
            return None
            
        parts = expression.split()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1 - —Å–ª–æ–≤–æ2 + —Å–ª–æ–≤–æ3
        if len(parts) == 5 and parts[1] == '-' and parts[3] == '+':
            try:
                word1, word2, word3 = parts[0], parts[2], parts[4]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ
                for word in [word1, word2, word3]:
                    if not self._word_in_vocabulary(word):
                        raise Exception(f"–°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ")
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∞–Ω–∞–ª–æ–≥–∏—é
                if hasattr(self.current_model, 'wv'):
                    result = self.current_model.wv.most_similar(
                        positive=[word3, word2],
                        negative=[word1],
                        topn=topn
                    )
                else:
                    # –î–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø—Ä—è–º–æ–≥–æ –º–µ—Ç–æ–¥–∞ most_similar
                    vec1 = self._get_word_vector(word1)
                    vec2 = self._get_word_vector(word2) 
                    vec3 = self._get_word_vector(word3)
                    
                    if vec1 is None or vec2 is None or vec3 is None:
                        raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤")
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º: word3 + word2 - word1
                    result_vector = vec3 + vec2 - vec1
                    
                    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
                    if hasattr(self.current_model, 'wv'):
                        result = self.current_model.wv.similar_by_vector(result_vector, topn=topn)
                    else:
                        raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏")
                
                return result
            except Exception as e:
                raise Exception(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è: {e}")
        else:
            raise Exception("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: —Å–ª–æ–≤–æ1 - —Å–ª–æ–≤–æ2 + —Å–ª–æ–≤–æ3")
    
    def _display_arithmetic_results(self, expression, results):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏"""
        st.success("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏—è:")
        
        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        df = pd.DataFrame(results, columns=["–°–ª–æ–≤–æ", "–°—Ö–æ–¥—Å—Ç–≤–æ"])
        st.dataframe(df, use_container_width=True)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        fig, ax = plt.subplots(figsize=(10, 6))
        words = [word for word, score in results]
        scores = [score for word, score in results]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(words)))
        bars = ax.barh(words, scores, color=colors)
        
        ax.set_xlabel("–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        ax.set_title(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {expression}")
        ax.grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar in bars:
            width = bar.get_width()
            ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                   f'{width:.3f}', ha='left', va='center')
        
        st.pyplot(fig)
    
    def render_semantic_similarity(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∞–Ω–∞–ª–∏–∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        st.header("üìä –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞")
        
        if not self.current_model:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ.")
            return
        
        col1, col2 = st.columns(2)
        
        with col1:
            word1 = st.text_input("–ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ:", value="—Ä–æ—Å—Å–∏—è", key="sim_word1")
            word2 = st.text_input("–í—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ:", value="–º–æ—Å–∫–≤–∞", key="sim_word2")
            
            if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ", key="sim_btn"):
                if word1 and word2:
                    try:
                        similarity = self._compute_similarity(word1, word2)
                        self._display_similarity_results(word1, word2, similarity)
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {e}")
        
        with col2:
            st.markdown("### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø–∞—Ä—ã")
            pairs = [
                ("—Ä–æ—Å—Å–∏—è", "–º–æ—Å–∫–≤–∞"),
                ("–∫–∏–Ω–æ", "—Ñ–µ—Å—Ç–∏–≤–∞–ª—å"),
                ("–∫—É–ª—å—Ç—É—Ä–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ"),
                ("–ø—É—Ç–∏–Ω", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"),
                ("—Ñ—É—Ç–±–æ–ª", "—Å–ø–æ—Ä—Ç"),
                ("—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–¥–µ–Ω—å–≥–∏")
            ]
            
            for w1, w2 in pairs:
                if st.button(f"{w1} - {w2}", key=f"pair_{w1}_{w2}"):
                    st.session_state.sim_word1 = w1
                    st.session_state.sim_word2 = w2
    
    def _display_similarity_results(self, word1, word2, similarity):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        # –î–∞—Ç—á–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞
        fig = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=similarity,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': f"–°—Ö–æ–¥—Å—Ç–≤–æ: {word1} - {word2}"},
            gauge={
                'axis': {'range': [0, 1]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 0.3], 'color': "lightcoral"},
                    {'range': [0.3, 0.7], 'color': "lightyellow"},
                    {'range': [0.7, 1], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 0.9
                }
            }
        ))
        
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
        if similarity > 0.7:
            st.success("‚úÖ –í—ã—Å–æ–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        elif similarity > 0.4:
            st.warning("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        else:
            st.error("‚ùå –ù–∏–∑–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
    
    def render_nearest_neighbors(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"""
        st.header("üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
        
        if not self.current_model:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞.")
            return
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            word = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–æ:", value="–∫—É–ª—å—Ç—É—Ä–∞", key="neighbors_word")
            topn = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π:", 1, 20, 10, key="neighbors_topn")
            
            if st.button("–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞", key="neighbors_btn"):
                if word:
                    with st.spinner("–ü–æ–∏—Å–∫..."):
                        try:
                            neighbors = self._find_nearest_neighbors(word, topn)
                            self._display_neighbors_results(word, neighbors)
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞: {e}")
        
        with col2:
            st.markdown("### –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤")
            test_words = ["–∫—É–ª—å—Ç—É—Ä–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–∫–∏–Ω–æ", "—Ä–æ—Å—Å–∏—è", "–ø—É—Ç–∏–Ω", "—Ñ—É—Ç–±–æ–ª", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"]
            
            for test_word in test_words:
                if st.button(test_word, key=f"btn_{test_word}"):
                    st.session_state.neighbors_word = test_word
    
    def _find_nearest_neighbors(self, word, topn):
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π"""
        if not self.current_model:
            raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        if not self._word_in_vocabulary(word):
            raise Exception(f"–°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ")
        
        return self._get_most_similar(word, topn)
    
    def _display_neighbors_results(self, word, neighbors):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–µ–π"""
        if not neighbors:
            st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
            return
            
        st.success(f"‚úÖ –°–ª–æ–≤–∞, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ '{word}':")
        
        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        df = pd.DataFrame(neighbors, columns=["–°–ª–æ–≤–æ", "–°—Ö–æ–¥—Å—Ç–≤–æ"])
        st.dataframe(df, use_container_width=True)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        fig, ax = plt.subplots(figsize=(12, 6))
        
        words = [word for word, score in neighbors]
        scores = [score for word, score in neighbors]
        y_pos = np.arange(len(words))
        
        bars = ax.barh(y_pos, scores)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(words)
        ax.set_xlabel("–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        ax.set_title(f"–°–ª–æ–≤–∞, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ '{word}'")
        ax.grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                   f'{width:.3f}', ha='left', va='center')
        
        st.pyplot(fig)

    def render_model_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö"""
        st.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö")
        
        if not self.models:
            st.warning("–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            return
        
        # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        st.subheader("üìä –û–±–∑–æ—Ä –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")
        
        model_info = []
        for model_name, model in self.models.items():
            info = {
                "–ú–æ–¥–µ–ª—å": model_name,
                "–¢–∏–ø": self._get_model_type(model_name),
                "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å": self._get_model_vector_size_for(model),
                "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞": self._get_model_architecture(model_name),
                "–°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ": len(self.vocab)
            }
            model_info.append(info)
        
        df_models = pd.DataFrame(model_info)
        st.dataframe(df_models, use_container_width=True)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        st.subheader(f"üîç –î–µ—Ç–∞–ª–∏ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏: {self.current_model_name}")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å", f"{self._get_model_vector_size()}D")
            st.metric("–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è", f"{len(self.vocab)}")
        
        with col2:
            st.metric("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", self._get_model_architecture(self.current_model_name))
            st.metric("–û–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "8")
        
        with col3:
            st.metric("–≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è", "50")
            st.metric("Min Count", "1")
        
        # –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        st.subheader("üîç –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏")
        
        test_words = ["–∫—É–ª—å—Ç—É—Ä–∞", "—Ä–æ—Å—Å–∏—è", "–∫–∏–Ω–æ", "–ø—É—Ç–∏–Ω", "—Ñ—É—Ç–±–æ–ª", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"]
        for word in test_words:
            if self._word_in_vocabulary(word):
                try:
                    similar = self._get_most_similar(word, topn=3)
                    if similar:
                        st.write(f"**{word}**: {[w for w, s in similar]}")
                    else:
                        st.write(f"**{word}**: –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
                except:
                    st.write(f"**{word}**: –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
    
    def _get_model_type(self, model_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏"""
        if "word2vec" in model_name.lower():
            return "Word2Vec"
        elif "fasttext" in model_name.lower():
            return "FastText"
        elif "doc2vec" in model_name.lower():
            return "Doc2Vec"
        else:
            return "Unknown"
    
    def _get_model_architecture(self, model_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏"""
        if "skip-gram" in model_name.lower() or "sg" in model_name.lower():
            return "Skip-gram"
        elif "cbow" in model_name.lower():
            return "CBOW"
        elif "dm" in model_name.lower():
            return "PV-DM"
        elif "dbow" in model_name.lower():
            return "PV-DBOW"
        else:
            return "Unknown"
    
    def _get_model_vector_size_for(self, model) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if hasattr(model, 'vector_size'):
            return model.vector_size
        elif hasattr(model, 'wv') and hasattr(model.wv, 'vector_size'):
            return model.wv.vector_size
        else:
            return 0
    
    def render_dashboard(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –¥–∞—à–±–æ—Ä–¥–∞"""
        st.set_page_config(
            page_title="–ê–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤",
            page_icon="üîç",
            layout="wide"
        )
        
        st.title("üîç –ê–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")
        st.markdown("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤")
        
        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
        viz_method, num_words = self.render_sidebar()
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        tab1, tab2, tab3 = st.tabs([
            "üßÆ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞", 
            "üîç –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞",
            "üìä –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", 
        ])
        
        with tab1:
            self.render_vector_arithmetic()
        
        with tab2:
            self.render_nearest_neighbors()
        
        with tab3:
            self.render_semantic_similarity()

# –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Streamlit
def main():
    try:
        explorer = VectorSpaceExplorer()
        explorer.render_dashboard()
    except Exception as e:
        st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()